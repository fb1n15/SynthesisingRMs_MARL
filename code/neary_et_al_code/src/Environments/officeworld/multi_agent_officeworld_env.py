import random, math, os
import numpy as np
from enum import Enum

import sys
sys.path.append('../')
sys.path.append('../../')
from reward_machines.sparse_reward_machine import SparseRewardMachine

"""
Enum with the actions that the agent can execute
"""
class Actions(Enum):
    up    = 0 # move up
    right = 1 # move right
    down  = 2 # move down
    left  = 3 # move left
    none  = 4 # none or pick up or drop

class MultiAgentOfficeWorldEnv:

    def __init__(self, rm_file, env_settings):
        """
        Initialize environment.

        Parameters
        ----------
        rm_file : string
            File path leading to the text file containing the reward machine
            encoding this environment's reward function.
        agent_id : int
            Index {0,1} indicating which agent
        env_settings : dict
            Dictionary of environment settings
        strategy_rm : boolean
            True if we are using the RMs generated by MCMAS and False if we are using the handcrafted ones.
        nonmarkovian : boolean
            True if we are considering the experiment in which the yellow button must be pressed twice to be activated.
        """
        self.env_settings = env_settings
        self._load_map()
        self.reward_machine = SparseRewardMachine(rm_file)

        self.u = self.reward_machine.get_initial_state()
        self.last_action = np.full(2, -1, dtype=int) #Initialize last action with garbage values

        self.blue_button_pushed = False
        self.orange_button_pushed = False

    def _load_map(self):
        """
        Initialize the environment.
        """
        self.Nr = self.env_settings['Nr']
        self.Nc = self.env_settings['Nc']

        initial_states = self.env_settings['initial_states']

        # Define Initial states of all agents
        self.s_i = np.full(2, -1, dtype=int)
        self.s_i[0] = initial_states[0]
        self.s_i[1] = initial_states[1]

        # Populate the map with markers
        self.objects = {}
        self.objects[self.env_settings['office']] = 'o'
        self.objects[self.env_settings['coffee']] = 'c'
        self.objects[self.env_settings['blue_button']] = 'bb'
        self.objects[self.env_settings['orange_button']] = 'ob'
        self.blueorange_tiles = self.env_settings['blueorange_tiles']

        self.p = self.env_settings['p']

        self.num_states = self.Nr * self.Nc

        actions = np.array(
            [Actions.up.value, Actions.right.value, Actions.left.value, Actions.down.value, Actions.none.value],
            dtype=int)
        self.actions = np.full((2, len(actions)), -2, dtype=int)
        self.actions[0] = actions
        self.actions[1] = actions

        # Define forbidden transitions corresponding to map edges
        self.forbidden_transitions = set()

        wall_locations = self.env_settings['walls']

        for row in range(self.Nr):
            self.forbidden_transitions.add((row, 0, Actions.left)) # If in left-most column, can't move left.
            self.forbidden_transitions.add((row, self.Nc - 1, Actions.right)) # If in right-most column, can't move right.
        for col in range(self.Nc):
            self.forbidden_transitions.add((0, col, Actions.up)) # If in top row, can't move up
            self.forbidden_transitions.add((self.Nr - 1, col, Actions.down)) # If in bottom row, can't move down

        # Restrict agent from having the option of moving "into" a wall
        for i in range(len(wall_locations)):
            (row, col) = wall_locations[i]
            self.forbidden_transitions.add((row, col + 1, Actions.left))
            self.forbidden_transitions.add((row, col - 1, Actions.right))
            self.forbidden_transitions.add((row + 1, col, Actions.up))
            self.forbidden_transitions.add((row - 1, col, Actions.down))

    def environment_step(self, s, a):
        """
        Execute collective action a from collective state s. Return the resulting reward,
        mdp label, and next state. Update the last action taken by each agent.

        Parameters
        ----------
        s : numpy integer array
            Array of integers representing the environment states of the various agents.
            s[id] represents the state of the agent indexed by index "id".
        a : numpy integer array
            Array of integers representing the actions selected by the various agents.
            a[id] represents the desired action to be taken by the agent indexed by "id.

        Outputs
        -------
        r : float
            Reward achieved by taking action a from state s.
        l : string
            MDP label emitted this step.
        s_next : numpy integer array
            Array of indeces of next team state.
        """
        s_next = np.full(2, -1, dtype=int)

        for i in range(2):
            s_next[i], last_action = self.get_next_state(s[i], a[i], i)
            self.last_action[i] = last_action

        l = self.get_mdp_label(s, s_next, self.u)
        r = 0

        for e in l:
            # Get the new reward machine state and the reward of this step
            u2 = self.reward_machine.get_next_state(self.u, e)
            r = r + self.reward_machine.get_reward(self.u, u2)
            # Update the reward machine state
            self.u = u2

        return r, l, s_next

    def get_next_state(self, s, a, agent_id):
        """
        Get the next state in the environment given action a is taken from state s.
        Update the last action that was truly taken due to MDP slip.

        Parameters
        ----------
        s : int
            Index of the current state.
        a : int
            Action to be taken from state s.

        Outputs
        -------
        s_next : int
            Index of the next state.
        last_action : int
            Last action the agent truly took because of slip probability.
        """
        slip_p = [self.p, (1 - self.p) / 2, (1 - self.p) / 2]
        check = random.random()

        row, col = self.get_state_description(s)

        # up    = 0
        # right = 1
        # down  = 2
        # left  = 3
        # stay  = 4

        if (check <= slip_p[0]) or (a == Actions.none.value):
            a_ = a

        elif (check > slip_p[0]) & (check <= (slip_p[0] + slip_p[1])):
            if a == 0:
                a_ = 3
            elif a == 2:
                a_ = 1
            elif a == 3:
                a_ = 2
            elif a == 1:
                a_ = 0

        else:
            if a == 0:
                a_ = 1
            elif a == 2:
                a_ = 3
            elif a == 3:
                a_ = 0
            elif a == 1:
                a_ = 2

        action_ = Actions(a_)
        if (row, col, action_) not in self.forbidden_transitions:
            if action_ == Actions.up:
                row -= 1
            if action_ == Actions.down:
                row += 1
            if action_ == Actions.left:
                col -= 1
            if action_ == Actions.right:
                col += 1

        s_next = self.get_state_from_description(row, col)

        # If the two buttons haven't been pressed in order yet, don't allow the agent into the colored region
        if self.u == 0 or self.u == 1:
            if (row, col) in self.blueorange_tiles:
                s_next = s

        last_action = a_
        return s_next, last_action

    def get_state_from_description(self, row, col):
        """
        Given a (row, column) index description of gridworld location, return
        index of corresponding state.

        Parameters
        ----------
        row : int
            Index corresponding to the row location of the state in the gridworld.
        col : int
            Index corresponding to the column location of the state in the gridworld.

        Outputs
        -------
        s : int
            The index of the gridworld state corresponding to location (row, col).
        """
        return self.Nc * row + col

    def get_state_description(self, s):
        """
        Return the row and column indeces of state s in the gridworld.

        Parameters
        ----------
        s : int
            Index of the gridworld state.

        Outputs
        -------
        row : int
            The row index of state s in the gridworld.
        col : int
            The column index of state s in the gridworld.
        """
        row = np.floor_divide(s, self.Nc)
        col = np.mod(s, self.Nc)

        return (row, col)

    def get_actions(self, id):
        """
        Returns the list with the actions that a particular agent can perform.

        Parameters
        ----------
        id : int
            Index of the agent whose initial state is being queried.
        """
        return np.copy(self.actions[id])

    def get_last_action(self, id):
        """
        Returns a particular agent's last action.

        Parameters
        ----------
        id : int
            Index of the agent whose initial state is being queried.
        """
        return self.last_action[id]

    def get_team_action_array(self):
        """
        Returns the available actions of the entire team.

        Outputs
        -------
        actions : (num_agents x num_actions) numpy integer array
        """
        return np.copy(self.actions)

    def get_initial_state(self, id):
        """
        Returns the initial state of a particular agent.

        Parameters
        ----------
        id : int
            Index of the agent whose initial state is being queried.
        """
        return self.s_i[id]

    def get_initial_team_state(self):
        """
        Return the intial state of the collective multi-agent team.

        Outputs
        -------
        s_i : numpy integer array
            Array of initial state indices for the agents in the experiment.
        """
        return np.copy(self.s_i)

    def get_next_agent_option(self, agent_id, rm_state):
        """
        Return the next meta-action of the agent specified by agent_id whose RM is in state rm_state.

        Parameters
        ------
        agent_id : int
            Id of the agent.
        rm_state : int
            State of the agent's RM.
        Returns
        ------
        The name of the next option to be performed by the agent.
        """
        if agent_id == 0:
            if rm_state in [0, 1]:
                return 'c'
            elif rm_state == 2:
                return 'o'
            else:
                return 'w' + str(agent_id + 1)
        elif agent_id == 1:
            if rm_state == 0:
                return 'bb'
            elif rm_state == 1:
                return 'bob'
            else:
                return 'w' + str(agent_id + 1)

    def get_mdp_label(self, s, s_next, u):
        """
        Get the mdp label resulting from transitioning from state s to state s_next.

        Parameters
        ----------
        s : numpy integer array
            Array of integers representing the environment states of the various agents.
            s[id] represents the state of the agent indexed by index "id".
        s_next : numpy integer array
            Array of integers representing the next environment states of the various agents.
            s_next[id] represents the next state of the agent indexed by index "id".
        u : int
            Index of the reward machine state

        Outputs
        -------
        l : string
            MDP label resulting from the state transition.
        """

        l = []

        agent1 = 0
        agent2 = 1

        prev_row1, prev_col1 = self.get_state_description(s[agent1])
        row1, col1 = self.get_state_description(s_next[agent1])
        prev_row2, prev_col2 = self.get_state_description(s[agent2])
        row2, col2 = self.get_state_description(s_next[agent2])

        if (row1, col1) in self.env_settings['decorations'] or (row2, col2) in self.env_settings['decorations']:
            return ['d']

        if u == 0:
            if (prev_row2, prev_col2) == self.env_settings['blue_button'] and \
                (row2, col2) == self.env_settings['blue_button']:
                l.append('bb')
        if u == 1:
            if (prev_row2, prev_col2) == self.env_settings['orange_button'] and \
                (row2, col2) == self.env_settings['orange_button']:
                l.append('bob')
        if u == 2:
            if (prev_row1, prev_col1) == self.env_settings['coffee'] and \
                (row1, col1) == self.env_settings['coffee']:
                l.append('c')
        if u == 3:
            if (prev_row1, prev_col1) == self.env_settings['office'] and \
                (row1, col1) == self.env_settings['office']:
                l.append('o')

        return l

    def get_options_list(self, agent_id):
        """
        Return a list of strings representing the possible options for each agent.

        Input
        -----
        agent_id : int
            The id of the agent whose option list is to be returned.

        Output
        ------
        options_list : list
            list of strings representing the options avaialble to the agent.
        """
        if agent_id == 0:
            return ['c', 'o']
        elif agent_id == 1:
            return ['bb', 'bob']

    def show(self, s):
        """
        Create a visual representation of the current state of the gridworld.

        Parameters
        ----------
        s : int
            Index of the current state
        """
        display = np.zeros((self.Nr, self.Nc))

        # Display the locations of the walls
        for loc in self.env_settings['walls']:
            display[loc] = -1

        display[self.env_settings['blue_button']] = 9
        display[self.env_settings['orange_button']] = 9
        display[self.env_settings['office']] = 9
        display[self.env_settings['coffee']] = 9

        for loc in self.blueorange_tiles:
            display[loc] = 5

        for loc in self.env_settings['decorations']:
            display[loc] = 8

        # Display the agents
        for i in range(2):
            row, col = self.get_state_description(s[i])
            display[row, col] = i + 1

        print(display)


def play():
    base_file_dir = os.path.abspath(os.path.join(os.getcwd(), '../../..'))
    rm_string = os.path.join(base_file_dir, 'experiments', 'officeworld', 'team_officeworld_rm.txt')

    # Set the environment settings for the experiment
    env_settings = dict()
    '''env_settings['Nr'] = 11
    env_settings['Nc'] = 15
    env_settings['initial_states'] = [30, 2]
    env_settings['blue_button'] = (1, 1)
    env_settings['orange_button'] = (1, 13)
    env_settings['walls'] = [(0, 3), (0, 7), (0, 11),
                             (2, 3), (2, 7), (2, 11),
                             (3, 0), (3, 2), (3, 3), (3, 4), (3, 6), (3, 7), (3, 8), (3, 10), (3, 11), (3, 12), (3, 14),
                             (4, 3), (4, 7), (4, 11),
                             (6, 3), (6, 7), (6, 11),
                             (7, 0), (7, 2), (7, 3), (7, 4), (7, 6), (7, 7), (7, 8), (7, 10), (7, 11), (7, 12), (7, 14),
                             (8, 3), (8, 7), (8, 11),
                             (10, 3), (10, 7), (10, 11)]
    env_settings['decorations'] = [(1, 5), (1, 9),
                                   (5, 1), (5, 9), (5, 13),
                                   (9, 1), (9, 5), (9, 9), (9, 13)]
    env_settings['blueorange_tiles'] = [(3, 13), (5, 11), (7, 13)]
    env_settings['office'] = (5, 5)
    env_settings['coffee'] = (4, 13)'''

    env_settings['Nr'] = 7
    env_settings['Nc'] = 7
    env_settings['initial_states'] = [14, 2]
    env_settings['blue_button'] = (1, 1)
    env_settings['orange_button'] = (1, 6)
    env_settings['walls'] = [(0, 3),
                             (2, 3),
                             (3, 0), (3, 2), (3, 3), (3, 4), (3, 6),
                             (4, 3),
                             (6, 3)]
    env_settings['decorations'] = [(1, 5),
                                   (5, 1)]
    env_settings['blueorange_tiles'] = [(3, 1), (5, 3)]
    env_settings['office'] = (5, 5)
    env_settings['coffee'] = (6, 2)

    env_settings['p'] = 0.98

    game = MultiAgentOfficeWorldEnv(rm_string, env_settings)

    # User inputs
    str_to_action = {"w": Actions.up.value, "d": Actions.right.value, "s": Actions.down.value, "a": Actions.left.value,
                     "x": Actions.none.value}

    s = game.get_initial_team_state()
    print(s)

    while True:
        # Showing game
        game.show(s)

        # Getting action
        a = np.full(2, -1, dtype=int)

        for i in range(2):
            print('\nAction{}?'.format(i + 1), end='')
            usr_inp = input()
            print()

            if not (usr_inp in str_to_action):
                print('forbidden action')
                a[i] = str_to_action['x']
            else:
                print(str_to_action[usr_inp])
                a[i] = str_to_action[usr_inp]

        r, l, s = game.environment_step(s, a)

        print("---------------------")
        print("Next States: ", s)
        print("Label: ", l)
        print("Reward: ", r)
        print("RM state: ", game.u)
        print("---------------------")

        if game.reward_machine.is_terminal_state(game.u) or 'd' in l:  # Game Over
            break
    game.show(s)


# This code allow to play a game (for debugging purposes)
if __name__ == '__main__':
    play()